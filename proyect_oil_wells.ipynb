{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OilyGiant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project at OilyGiant, we're tackling a practical challenge: finding the best spots to drill 200 new oil wells. It's all about using data smartly to make profitable decisions in a competitive industry. \n",
    "\n",
    "We have geological data for oil samples in three regions. Parameters of each oil well are already known. We need to build a model that will help to pick the region with the highest profit margin, and predict the amount of oil that can be extracted from the wells.\n",
    "\n",
    "We'll analyze potential profit and risks using the Bootstrapping technique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.- Data Analysis: Dig into well data from three regions to understand oil quality and reserve volumes.\n",
    "\n",
    "2.- Model Development: Build a model to predict reserve volumes in new wells.\n",
    "\n",
    "3.- Well Selection: Use the model to pick the most promising well locations. \n",
    "\n",
    "4.- Region Selection: Identify the region with the highest overall profitability potential.\n",
    "\n",
    "5.- Risk Assessment: Apply bootstrapping techniques to analyze potential profits and risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation And Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('datasets/geo_data_0.csv')\n",
    "df1 = pd.read_csv('datasets/geo_data_1.csv')\n",
    "df2 = pd.read_csv('datasets/geo_data_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id        f0        f1        f2     product\n",
      "0  txEyH  0.705745 -0.497823  1.221170  105.280062\n",
      "1  2acmU  1.334711 -0.340164  4.365080   73.037750\n",
      "2  409Wp  1.022732  0.151990  1.419926   85.265647\n",
      "3  iJLyR -0.032172  0.139033  2.978566  168.620776\n",
      "4  Xdl7t  1.988431  0.155413  4.751769  154.036647\n",
      "      id         f0         f1        f2     product\n",
      "0  kBEdx -15.001348  -8.276000 -0.005876    3.179103\n",
      "1  62mP7  14.272088  -3.475083  0.999183   26.953261\n",
      "2  vyE1P   6.263187  -5.948386  5.001160  134.766305\n",
      "3  KcrkZ -13.081196 -11.506057  4.999415  137.945408\n",
      "4  AHL4O  12.702195  -8.147433  5.004363  134.766305\n",
      "      id        f0        f1        f2     product\n",
      "0  fwXo0 -1.146987  0.963328 -0.828965   27.758673\n",
      "1  WJtFt  0.262778  0.269839 -2.530187   56.069697\n",
      "2  ovLUW  0.194587  0.289035 -5.586433   62.871910\n",
      "3  q6cA6  2.236060 -0.553760  0.930038  114.572842\n",
      "4  WPMUX -0.515993  1.716266  5.899011  149.600746\n"
     ]
    }
   ],
   "source": [
    "print(df0.head())\n",
    "print(df1.head())\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cheking the head of the dataframes, we can see that the columns are named as follows: id, f0, f1, f2, and product. The id column contains the unique well identifier. The values of features f0, f1, and f2 are also features of points (their specific meaning is unimportant, but the features themselves are significant). The target is the product column, which contains the volume of reserves in the oil well (in thousand barrels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use .info() to check the general information of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   id       100000 non-null  object \n",
      " 1   f0       100000 non-null  float64\n",
      " 2   f1       100000 non-null  float64\n",
      " 3   f2       100000 non-null  float64\n",
      " 4   product  100000 non-null  float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 3.8+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   id       100000 non-null  object \n",
      " 1   f0       100000 non-null  float64\n",
      " 2   f1       100000 non-null  float64\n",
      " 3   f2       100000 non-null  float64\n",
      " 4   product  100000 non-null  float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 3.8+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   id       100000 non-null  object \n",
      " 1   f0       100000 non-null  float64\n",
      " 2   f1       100000 non-null  float64\n",
      " 3   f2       100000 non-null  float64\n",
      " 4   product  100000 non-null  float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 3.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df0.info())\n",
    "print(df1.info())\n",
    "print(df2.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the three dataframes have 100,000 rows and 5 columns. There are no missing values in the dataframes. The data types of the columns are float64 except for the id column which is a type object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as we can see, the data looks ready for further analysis. But we are going to check if there are any duplicates in the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in f0 =  10\n",
      "Duplicates in f1 =  4\n",
      "Duplicates in f2 =  4\n"
     ]
    }
   ],
   "source": [
    "print('Duplicates in f0 = ', df0.duplicated(subset='id').sum())\n",
    "print('Duplicates in f1 = ', df1.duplicated(subset='id').sum())\n",
    "print('Duplicates in f2 = ', df2.duplicated(subset='id').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droping duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we should not have any duplicates in the dataframes, because each row represents a unique well, we are going to eliminate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df0.drop_duplicates(subset='id')\n",
    "df1 = df1.drop_duplicates(subset='id')\n",
    "df2 = df2.drop_duplicates(subset='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if there are still duplicates in the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates in f0 =  0\n",
      "Duplicates in f1 =  0\n",
      "Duplicates in f2 =  0\n"
     ]
    }
   ],
   "source": [
    "print('Duplicates in f0 = ', df0.duplicated(subset='id').sum())\n",
    "print('Duplicates in f1 = ', df1.duplicated(subset='id').sum())\n",
    "print('Duplicates in f2 = ', df2.duplicated(subset='id').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that there are no duplicates in the dataframes, so we can continue with the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into a training set and validation sets in geo_data_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split the data into a training set and validation sets, using the train_test_split() function from the sklearn.model_selection library.\n",
    "\n",
    "The target values are going to be the column \"product\", and the rest of the columns are going to be the features.\n",
    "\n",
    "We also gonna drop the id column, because is a type object and we don't need it for the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features and target features for geo_data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df0.drop(['id', 'product'], axis=1)\n",
    "y = df0['product']  \n",
    "\n",
    "features_train_0, features_valid_0, target_train_0, target_valid_0 = train_test_split(X, y, test_size=0.25, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only going to use linear regression to train the model, since we are not going to use any other model in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training for geo_data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  37.853527328872964\n",
      "Mean =  92.15820490940044\n",
      "Predicted mean =  92.7891563828062\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(features_train_0, target_train_0)\n",
    "predicted_valid_0 = model.predict(features_valid_0)\n",
    "print('RMSE = ', sqrt(mean_squared_error(target_valid_0, predicted_valid_0)))\n",
    "print('Mean = ', target_valid_0.mean())\n",
    "print('Predicted mean = ', predicted_valid_0.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features and target features for geo_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df1.drop(['id', 'product'], axis=1)\n",
    "y = df1['product']\n",
    "\n",
    "features_train_1, features_valid_1, target_train_1, target_valid_1 = train_test_split(X, y, test_size=0.25, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training for geo_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  0.8920592647717042\n",
      "Mean =  69.18604400957675\n",
      "Predicted mean =  69.1783195703043\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LinearRegression()\n",
    "model.fit(features_train_1, target_train_1)\n",
    "predicted_valid_1 = model.predict(features_valid_1)\n",
    "print('RMSE = ', sqrt(mean_squared_error(target_valid_1, predicted_valid_1)))\n",
    "print('Mean = ', target_valid_1.mean())\n",
    "print('Predicted mean = ', predicted_valid_1.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features and target features for geo_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df2.drop(['id', 'product'], axis=1)\n",
    "y = df2['product']\n",
    "\n",
    "features_train_2, features_valid_2, target_train_2, target_valid_2 = train_test_split(X, y, test_size=0.25, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training for geo_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE =  40.07585073246016\n",
      "Mean =  94.7851093536914\n",
      "Predicted mean =  94.86572480562035\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LinearRegression()\n",
    "model.fit(features_train_2, target_train_2)\n",
    "predicted_valid_2 = model.predict(features_valid_2)\n",
    "print('RMSE = ', sqrt(mean_squared_error(target_valid_2, predicted_valid_2)))\n",
    "print('Mean = ', target_valid_2.mean())\n",
    "print('Predicted mean = ', predicted_valid_2.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the RMSE of the model geo_data_0 is 37.579422, the RMSE of the model geo_data_1 is 0.893099, and the RMSE of the model geo_data_2 is 40.029709.\n",
    "\n",
    "In this case, the model geo_data_1 is the best one, because it has the lowest RMSE.\n",
    "\n",
    "Also the mean and predicted values are very close together, wich is good for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Profit calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Break-even point (in units): 111.11111111111111\n",
      "Average reserve in Region 0: 92.49968421774354 units\n",
      "Average reserve in Region 1: 68.82391591804064 units\n",
      "Average reserve in Region 2: 94.99834211933378 units\n"
     ]
    }
   ],
   "source": [
    "total_investment = 100000000 \n",
    "number_of_wells = 200\n",
    "revenue_per_unit = 4500 \n",
    "\n",
    "break_even_units = total_investment / (number_of_wells * revenue_per_unit)\n",
    "print('Break-even point (in units):', break_even_units)\n",
    "\n",
    "print('Average reserve in Region 0:', df0['product'].mean(), 'units')\n",
    "print('Average reserve in Region 1:', df1['product'].mean(), 'units')\n",
    "print('Average reserve in Region 2:', df2['product'].mean(), 'units')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the regions, on average, meet the break-even reserve volume of 111.11 units based on this initial analysis. This suggests that under the current investment plan and based on average reserves, the oil well development might not be profitable in any of the regions.\n",
    "\n",
    "But we are going to select the 200 wells with the highest values of predictions and calculate the profit for the obtained volume of reserves on each region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predicted profit for Region 0: 33651872.377002865\n",
      "Total predicted profit for Region 1: 24150866.966815114\n",
      "Total predicted profit for Region 2: 25012838.532820627\n"
     ]
    }
   ],
   "source": [
    "def calculate_profit(predictions, target_valid, revenue_per_unit, number_of_wells=200):\n",
    "    # We convert predictions to a Pandas Series and reset the index of target_valid so that they match.\n",
    "    target_valid = target_valid.reset_index(drop=True)\n",
    "    predictions = pd.Series(predictions).reset_index(drop=True)\n",
    "\n",
    "    # Sort and selection of top indices\n",
    "    sorted_indices = np.argsort(predictions)[::-1]\n",
    "    top_indices = sorted_indices[:number_of_wells]\n",
    "\n",
    "    # Total reserves and profit\n",
    "    total_reserves = np.sum(target_valid[top_indices])\n",
    "    total_profit = (total_reserves * revenue_per_unit) - total_investment\n",
    "    return total_profit\n",
    "\n",
    "# Now we apply this function to our predictions\n",
    "profit_region0 = calculate_profit(predicted_valid_0, target_valid_0, revenue_per_unit)\n",
    "profit_region1 = calculate_profit(predicted_valid_1, target_valid_1, revenue_per_unit)\n",
    "profit_region2 = calculate_profit(predicted_valid_2, target_valid_2, revenue_per_unit)\n",
    "\n",
    "print('Total predicted profit for Region 0:', profit_region0)\n",
    "print('Total predicted profit for Region 1:', profit_region1)\n",
    "print('Total predicted profit for Region 2:', profit_region2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the table above.\n",
    "\n",
    "- Region 0: Predicted profit of approximately $139.82 million.\n",
    "- Region 1: Predicted profit of approximately $124.86 million.\n",
    "- Region 2: Predicted profit of approximately $133.63 million.\n",
    "\n",
    "Region 0 shows the highest potential profit, followed closely by Region 2. Region 1, while still profitable, has a lower predicted profit compared to the other two regions.\n",
    "These results suggest that, based on the model's predictions, investing in oil wells in either Region 0 or Region 2 could be more profitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each region, we are going to:\n",
    "\n",
    "- Use the bootstrapping technique with 1000 samples to find the distribution of profit.\n",
    "- Find average profit, 95% confidence interval, and risk of losses. Loss is negative profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average profit for Region 0: 3740770.621899106\n",
      "95% confidence interval: [-1622671.77347309  9025539.73278304]\n",
      "Risk of loss: 6.4\n",
      "\n",
      "Average profit for Region 1: 4675197.310061187\n",
      "95% confidence interval: [ 431675.89325556 8880092.63048083]\n",
      "Risk of loss: 1.7000000000000002\n",
      "\n",
      "Average profit for Region 2: 3581893.245733123\n",
      "95% confidence interval: [-2186732.04780835  8645820.11408043]\n",
      "Risk of loss: 10.100000000000001\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_profit_analysis(predictions, target_valid, revenue_per_unit, n_bootstrap=1000, n_wells=500):\n",
    "    bootstrap_profits = []\n",
    "    target_valid = target_valid.reset_index(drop=True)\n",
    "    for _ in range(n_bootstrap):\n",
    "        indexes = np.random.choice(range(len(predictions)), size=n_wells, replace=True)\n",
    "        \n",
    "        indexed_predictions = predictions[indexes]\n",
    "        indexed_target_valid = target_valid[indexes]\n",
    "        \n",
    "        profit = calculate_profit(indexed_predictions, indexed_target_valid, revenue_per_unit)\n",
    "        bootstrap_profits.append(profit)\n",
    "\n",
    "\n",
    "    average_profit = np.mean(bootstrap_profits)\n",
    "    confidence_interval = np.percentile(bootstrap_profits, [2.5, 97.5])\n",
    "\n",
    "    risk_of_loss = np.mean(np.array(bootstrap_profits) < 0) * 100 \n",
    "    \n",
    "\n",
    "    return average_profit, confidence_interval, risk_of_loss\n",
    "\n",
    "\n",
    "avg_profit_0, ci_0, risk_0 = bootstrap_profit_analysis(predicted_valid_0, target_valid_0, revenue_per_unit)\n",
    "avg_profit_1, ci_1, risk_1 = bootstrap_profit_analysis(predicted_valid_1, target_valid_1, revenue_per_unit)\n",
    "avg_profit_2, ci_2, risk_2 = bootstrap_profit_analysis(predicted_valid_2, target_valid_2, revenue_per_unit)\n",
    "\n",
    "\n",
    "print('Average profit for Region 0:', avg_profit_0)\n",
    "print('95% confidence interval:', ci_0)\n",
    "print('Risk of loss:', risk_0)\n",
    "print()\n",
    "print('Average profit for Region 1:', avg_profit_1)\n",
    "print('95% confidence interval:', ci_1)\n",
    "print('Risk of loss:', risk_1)\n",
    "print()\n",
    "print('Average profit for Region 2:', avg_profit_2)\n",
    "print('95% confidence interval:', ci_2)\n",
    "print('Risk of loss:', risk_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results of the bootstrapping analysis, we can see that: \n",
    "\n",
    "- Region 1 has the highest average profit of approximately 47.84 Millions\n",
    "- Region 1 also has the lowest risk of losses, with an confidence interval (95%) of approximately .888 in the lower end and 8.958 millions in the upper end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we can see in all this analysis, the best region to drill for oil is the region 1, because it has the highest average profit of approximately 47.84 Millions and also has the lowest risk of losses, with 1.6% of loss.\n",
    "\n",
    "Also the confidence interval (95%) in the lower level is still positive compared with the other 2 regions, so it makes sense to take that into consideration.\n",
    "\n",
    "This was not seen in the initial analysis, where we saw that the region 0 was the best one, for potential profit.\n",
    "\n",
    "But doing the bootstrapping analysis, we see a more indeep result and we have more toosl to say that the region 1 is the best one, because it has the highest average profit, lower risk and a positive confidenec interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
